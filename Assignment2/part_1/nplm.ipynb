{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "import data_fnn as data\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_split(orig_corpus, dataset, n):\n",
    "    # This function breaks corpus into [context, target]\n",
    "    # For e.g., in trigram, the tensor returned would be [C(n-2), C(n-1), T]\n",
    "    ngram = []\n",
    "    data_len = len(dataset)\n",
    "    eos_id = corpus.dictionary.word2idx['<eos>']       \n",
    "    for i, tokenid in enumerate(dataset):\n",
    "        if i+n<data_len:\n",
    "            temp_gram = dataset[i:i+n+1].view(-1)\n",
    "            if eos_id in temp_gram[0:n]:\n",
    "                continue\n",
    "            ngram.append(temp_gram)\n",
    "    fin_ngram=torch.stack(ngram)\n",
    "    return fin_ngram\n",
    "\n",
    "def get_accuracy_from_log_probs(log_probs, labels):\n",
    "    probs = torch.exp(log_probs)\n",
    "    predicted_label = torch.argmax(probs, dim=1)\n",
    "    acc = (predicted_label == labels).float().mean()\n",
    "    return acc\n",
    "\n",
    "# helper function to evaluate model on dev data\n",
    "def evaluate(model, criterion, dataloader, gpu):\n",
    "    model.eval()\n",
    "\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        dev_st = time.time()\n",
    "        for it, data_tensor in enumerate(dataloader):\n",
    "            context_tensor = data_tensor[:,0:CONTEXT_SIZE]\n",
    "            target_tensor = data_tensor[:,CONTEXT_SIZE]\n",
    "            context_tensor, target_tensor = context_tensor.cuda(gpu), target_tensor.cuda(gpu)\n",
    "            log_probs = model(context_tensor)\n",
    "            mean_loss += criterion(log_probs, target_tensor).item()\n",
    "            mean_acc += get_accuracy_from_log_probs(log_probs, target_tensor)\n",
    "            count += 1\n",
    "            if it % 500 == 0: \n",
    "                print(\"Dev Iteration {} complete. Mean Loss: {}; Mean Acc:{}; Time taken (s): {}\".format(it, mean_loss / count, mean_acc / count, (time.time()-dev_st)))\n",
    "                dev_st = time.time()\n",
    "\n",
    "    return mean_acc / count, mean_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create parameters\n",
    "gpu = 0 \n",
    "# word vectors size\n",
    "EMBEDDING_DIM = 200\n",
    "CONTEXT_SIZE = 7\n",
    "BATCH_SIZE = 512\n",
    "tied = False\n",
    "# hidden units\n",
    "H = 100\n",
    "torch.manual_seed(42)\n",
    "learn_rate = 1e-3\n",
    "\n",
    "# check if gpu is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = './data/wikitext-2'\n",
    "corpus = data.Corpus(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = corpus.train\n",
    "val_set = corpus.valid\n",
    "test_set = corpus.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ngram = ngram_split(corpus, train_set, CONTEXT_SIZE)\n",
    "val_ngram = ngram_split(corpus, val_set, CONTEXT_SIZE)\n",
    "test_ngram = ngram_split(corpus, test_set, CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ngram, batch_size = BATCH_SIZE, shuffle=True)\n",
    "dev_loader = DataLoader(val_ngram, batch_size = BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_ngram, batch_size = BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using negative log-likelihood loss\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "vocab_len = len(corpus.dictionary)\n",
    "\n",
    "# create model\n",
    "# Using negative log-likelihood loss\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "# create model\n",
    "model = model.FNNModel(vocab_len, EMBEDDING_DIM, CONTEXT_SIZE, H, tied)\n",
    "\n",
    "# load it to gpu\n",
    "model.cuda(gpu)\n",
    "\n",
    "# using ADAM optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr = learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training model Epoch: 1 ---\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 5.859422206878662; Mean Acc:0.158203125; Time taken (s): 0.0431671142578125\n",
      "Epoch 1 complete! Development Accuracy: 0.16658324003219604; Development Loss: 5.977240417491306; Perplexity: 394.35062214570576\n",
      "Best development accuracy improved from 999 to 394.35062214570576, saving model...\n",
      "\n",
      "--- Training model Epoch: 2 ---\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 6.151486396789551; Mean Acc:0.171875; Time taken (s): 0.037268877029418945\n",
      "Epoch 2 complete! Development Accuracy: 0.17587989568710327; Development Loss: 5.844574175097725; Perplexity: 345.35544951498883\n",
      "Best development accuracy improved from 394.35062214570576 to 345.35544951498883, saving model...\n",
      "\n",
      "--- Training model Epoch: 3 ---\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 5.80184268951416; Mean Acc:0.1796875; Time taken (s): 0.04019021987915039\n",
      "Epoch 3 complete! Development Accuracy: 0.17962992191314697; Development Loss: 5.7813992933793505; Perplexity: 324.2125416205935\n",
      "Best development accuracy improved from 345.35544951498883 to 324.2125416205935, saving model...\n",
      "\n",
      "--- Training model Epoch: 4 ---\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 5.996655464172363; Mean Acc:0.171875; Time taken (s): 0.031208276748657227\n",
      "Epoch 4 complete! Development Accuracy: 0.18239617347717285; Development Loss: 5.75870568914847; Perplexity: 316.9378472347702\n",
      "Best development accuracy improved from 324.2125416205935 to 316.9378472347702, saving model...\n",
      "\n",
      "--- Training model Epoch: 5 ---\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 5.42747688293457; Mean Acc:0.19921875; Time taken (s): 0.015621662139892578\n",
      "Epoch 5 complete! Development Accuracy: 0.1829206943511963; Development Loss: 5.758518674156883; Perplexity: 316.8785806479788\n",
      "Best development accuracy improved from 316.9378472347702 to 316.8785806479788, saving model...\n",
      "\n",
      "--- Training model Epoch: 6 ---\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 5.72390079498291; Mean Acc:0.189453125; Time taken (s): 0.015621423721313477\n",
      "Epoch 6 complete! Development Accuracy: 0.18280380964279175; Development Loss: 5.768896188248288; Perplexity: 320.1841145310134\n",
      "Best development accuracy improved from 316.8785806479788 to 320.1841145310134, saving model...\n",
      "\n",
      "--- Training model Epoch: 7 ---\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 5.703252792358398; Mean Acc:0.181640625; Time taken (s): 0.015622377395629883\n",
      "Epoch 7 complete! Development Accuracy: 0.18282932043075562; Development Loss: 5.780745839530772; Perplexity: 324.00075289213004\n",
      "Best development accuracy improved from 320.1841145310134 to 324.00075289213004, saving model...\n",
      "\n",
      "--- Training model Epoch: 8 ---\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 5.836897850036621; Mean Acc:0.197265625; Time taken (s): 0.015589475631713867\n",
      "Epoch 8 complete! Development Accuracy: 0.18343758583068848; Development Loss: 5.808560677550056; Perplexity: 333.1392855824006\n",
      "Best development accuracy improved from 324.00075289213004 to 333.1392855824006, saving model...\n",
      "\n",
      "--- Training model Epoch: 9 ---\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 5.337359428405762; Mean Acc:0.197265625; Time taken (s): 0.015579462051391602\n",
      "Epoch 9 complete! Development Accuracy: 0.1828310489654541; Development Loss: 5.839456424117088; Perplexity: 343.5925212861922\n",
      "Best development accuracy improved from 333.1392855824006 to 343.5925212861922, saving model...\n",
      "\n",
      "--- Training model Epoch: 10 ---\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 5.873520374298096; Mean Acc:0.16015625; Time taken (s): 0.015619754791259766\n",
      "Epoch 10 complete! Development Accuracy: 0.18324129283428192; Development Loss: 5.8680441731756385; Perplexity: 353.55680771512266\n",
      "Best development accuracy improved from 343.5925212861922 to 353.55680771512266, saving model...\n",
      "\n",
      "--- Training model Epoch: 11 ---\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 6.0474982261657715; Mean Acc:0.166015625; Time taken (s): 0.015581369400024414\n",
      "Epoch 11 complete! Development Accuracy: 0.18197950720787048; Development Loss: 5.902614105831493; Perplexity: 365.99296277708083\n",
      "Best development accuracy improved from 353.55680771512266 to 365.99296277708083, saving model...\n",
      "\n",
      "--- Training model Epoch: 12 ---\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 5.780980587005615; Mean Acc:0.203125; Time taken (s): 0.01558828353881836\n",
      "Epoch 12 complete! Development Accuracy: 0.18112465739250183; Development Loss: 5.939855926416137; Perplexity: 379.8801948942115\n",
      "Best development accuracy improved from 365.99296277708083 to 379.8801948942115, saving model...\n",
      "\n",
      "--- Training model Epoch: 13 ---\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 5.551908493041992; Mean Acc:0.236328125; Time taken (s): 0.01562190055847168\n",
      "Epoch 13 complete! Development Accuracy: 0.18014687299728394; Development Loss: 5.976025294173848; Perplexity: 393.8717285258095\n",
      "Best development accuracy improved from 379.8801948942115 to 393.8717285258095, saving model...\n",
      "\n",
      "--- Training model Epoch: 14 ---\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 6.309244155883789; Mean Acc:0.166015625; Time taken (s): 0.015582084655761719\n",
      "Epoch 14 complete! Development Accuracy: 0.1793908327817917; Development Loss: 6.019829244776205; Perplexity: 411.5083225313716\n",
      "Best development accuracy improved from 393.8717285258095 to 411.5083225313716, saving model...\n",
      "\n",
      "--- Training model Epoch: 15 ---\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 6.30977201461792; Mean Acc:0.134765625; Time taken (s): 0.0155792236328125\n",
      "Epoch 15 complete! Development Accuracy: 0.1774296909570694; Development Loss: 6.0542744398117065; Perplexity: 425.9297556410686\n",
      "Best development accuracy improved from 411.5083225313716 to 425.9297556410686, saving model...\n",
      "\n",
      "--- Training model Epoch: 16 ---\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 6.032561779022217; Mean Acc:0.189453125; Time taken (s): 0.015587806701660156\n",
      "Epoch 16 complete! Development Accuracy: 0.17804257571697235; Development Loss: 6.090663920749318; Perplexity: 441.7145772887975\n",
      "Best development accuracy improved from 425.9297556410686 to 441.7145772887975, saving model...\n",
      "\n",
      "--- Training model Epoch: 17 ---\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 5.800414085388184; Mean Acc:0.2109375; Time taken (s): 0.015577077865600586\n",
      "Epoch 17 complete! Development Accuracy: 0.17599590122699738; Development Loss: 6.1333800513635985; Perplexity: 460.9917059317761\n",
      "Best development accuracy improved from 441.7145772887975 to 460.9917059317761, saving model...\n",
      "\n",
      "--- Training model Epoch: 18 ---\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 6.180757522583008; Mean Acc:0.193359375; Time taken (s): 0.015622138977050781\n",
      "Epoch 18 complete! Development Accuracy: 0.17554481327533722; Development Loss: 6.174681148745797; Perplexity: 480.4298138476931\n",
      "Best development accuracy improved from 460.9917059317761 to 480.4298138476931, saving model...\n",
      "\n",
      "--- Training model Epoch: 19 ---\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 6.098899841308594; Mean Acc:0.1875; Time taken (s): 0.0155792236328125\n",
      "Epoch 19 complete! Development Accuracy: 0.17428533732891083; Development Loss: 6.207775596867908; Perplexity: 496.5953934573874\n",
      "Best development accuracy improved from 480.4298138476931 to 496.5953934573874, saving model...\n",
      "\n",
      "--- Training model Epoch: 20 ---\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 6.114480972290039; Mean Acc:0.197265625; Time taken (s): 0.015583992004394531\n",
      "Epoch 20 complete! Development Accuracy: 0.17348410189151764; Development Loss: 6.245699549263174; Perplexity: 515.7899192083671\n",
      "Best development accuracy improved from 496.5953934573874 to 515.7899192083671, saving model...\n"
     ]
    }
   ],
   "source": [
    "# ------------------------- TRAIN & SAVE MODEL ------------------------\n",
    "best_acc = 0\n",
    "best_ppl = 999\n",
    "best_model_path = None\n",
    "for epoch in range(20):\n",
    "    st = time.time()\n",
    "    print(\"\\n--- Training model Epoch: {} ---\".format(epoch+1))\n",
    "    for it, data_tensor in enumerate(train_loader):\n",
    "        context_tensor = data_tensor[:,0:CONTEXT_SIZE]\n",
    "        target_tensor = data_tensor[:,CONTEXT_SIZE]\n",
    "#         print(context_tensor)\n",
    "#         print(target_tensor)\n",
    "        context_tensor, target_tensor = context_tensor.cuda(gpu), target_tensor.cuda(gpu)\n",
    "\n",
    "        # zero out the gradients from the old instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # get log probabilities over next words\n",
    "        log_probs = model(context_tensor)\n",
    "        # calculate current accuracy\n",
    "        acc = get_accuracy_from_log_probs(log_probs, target_tensor)\n",
    "\n",
    "        # compute loss function\n",
    "        loss = loss_function(log_probs, target_tensor)\n",
    "\n",
    "        # backward pass and update gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if it % 500 == 0: \n",
    "#             print(\"Training Iteration {} of epoch {} complete. Loss: {}; Acc:{}; Time taken (s): {}\".format(it, epoch, loss.item(), acc, (time.time()-st)))\n",
    "            st = time.time()\n",
    "\n",
    "    print(\"\\n--- Evaluating model on dev data ---\")\n",
    "    dev_acc, dev_loss = evaluate(model, loss_function, dev_loader, gpu)\n",
    "    ppl = math.exp(dev_loss)\n",
    "    print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}; Perplexity: {}\".format(epoch+1, dev_acc, dev_loss, ppl))\n",
    "    if ppl < best_ppl:\n",
    "        print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_ppl, ppl))\n",
    "        best_ppl = ppl\n",
    "        # set best model path\n",
    "        best_model_path = 'best_model_{}_gram_{}_{}H.dat'.format(CONTEXT_SIZE+1, epoch, H)\n",
    "        # saving best model\n",
    "        torch.save(model.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'best_model_8_gram_19_100H.dat'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = 'Experiment3.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(best_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Iteration 0 complete. Mean Loss: 6.155126094818115; Mean Acc:0.154296875; Time taken (s): 0.023903608322143555\n"
     ]
    }
   ],
   "source": [
    "test_acc, test_loss = evaluate(model, loss_function, test_loader, gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 506.15863358029986\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Perplexity:\", math.exp(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'Experiment4.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_fnn as data\n",
    "import model\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data.Corpus('./data/wikitext-2')\n",
    "ntokens = len(corpus.dictionary)\n",
    "vocab_len = len(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'Experiment1.pt'\n",
    "device = torch.device(\"cuda\")\n",
    "test_model = torch.load(model_path)\n",
    "# test_model.load_state_dict(torch.load('best_model_8_gram_11_200H.dat'))\n",
    "test_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = dict(test_model.named_children())\n",
    "embedding_size = l['embeddings'].embedding_dim # 200\n",
    "input_layer_dim = l['linear1'].in_features # 1400\n",
    "context_size = int(input_layer_dim/embedding_size) # 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_corpus = torch.cat((corpus.train, corpus.valid, corpus.test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_pos = random.randint(0, len(full_corpus)-context_size)\n",
    "seed_span = full_corpus[seed_pos:seed_pos+context_size] # Pick random span from corpus\n",
    "generated_text=seed_span.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for i in range(15):\n",
    "    with torch.no_grad():\n",
    "        output = model(generated_text[-7:])\n",
    "        word_id = torch.argmax(output, dim=1)\n",
    "        generated_text = torch.cat((generated_text,word_id))\n",
    "        print(generated_text[-8:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = generated_text.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, i in enumerate(generated_text):\n",
    "    if index == 7: \n",
    "        print(\" | \", end = \"\")\n",
    "    print(corpus.dictionary.idx2word[i], end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### the words of , as often as | much as the of the decade of the united states . <eos> of the united\n",
      "$$$ the words of , as often as not consisted of a stern lecture from on the\n",
      "### ten years later , the census counted | the only rectangular - day run of the two companies of the city of the\n",
      "$$$ ten years later , the census counted just 16 . 9 million , the remainder having\n",
      "### . also sought to involve the senate | of the assembly , who had a large amount of energy of the . <eos>\n",
      "$$$ . also sought to involve the senate in his government , but this was not entirely\n",
      "### . <eos> persisted on cruising ironclads for | the championship . <eos> of the season of the german , austrian and ottoman force\n",
      "$$$ . <eos> persisted on cruising ironclads for much longer . during the 1860s , the french\n",
      "### upstairs window . in preparation for s | the ship , and the other candidate cities in the city of the church s\n",
      "$$$ upstairs window . in preparation for s visit the free derry wall was painted white and\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for _ in range(5):\n",
    "    seed_pos = random.randint(0, len(corpus.test)-7)\n",
    "    seed_span = corpus.test[seed_pos:seed_pos+7] # Pick random span from corpus\n",
    "    generated_text=seed_span.to(device)\n",
    "    for i in range(15):\n",
    "        with torch.no_grad():\n",
    "            output = model(generated_text[-7:])\n",
    "            word_id = torch.argmax(output, dim=1)\n",
    "            generated_text = torch.cat((generated_text,word_id))\n",
    "            #print(generated_text[-8:])\n",
    "    sent = [corpus.dictionary.idx2word[i] for i in generated_text] \n",
    "    sent = sent[:7] + ['|']+sent[7:]\n",
    "    print(\"###\",' '.join(sent))\n",
    "    orig = [corpus.dictionary.idx2word[i] for i in corpus.test[seed_pos:seed_pos+len(sent)-7]] \n",
    "    print(\"$$$\",' '.join(orig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.linear2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.linear2.weight==model.embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.linear2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.embeddings.weight.detach().cpu()\n",
    "b = model.linear2.weight.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.linear2.weight.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sum(np.abs(a-b).numpy()))/(28744*200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(a.flatten().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(a.flatten().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
