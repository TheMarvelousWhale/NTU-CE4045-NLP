{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16761e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "574feee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_split(dataset, n):\n",
    "    # This function breaks corpus into [context, target]\n",
    "    # For e.g., in trigram, the tensor returned would be [C(n-2), C(n-1), T]\n",
    "    ngram = []\n",
    "    data_len = len(dataset)\n",
    "    for i, tokenid in enumerate(dataset):\n",
    "        if i+n<data_len:\n",
    "            ngram.append(dataset[i:i+n+1].view(-1))\n",
    "    fin_ngram=torch.stack(ngram)\n",
    "    return fin_ngram\n",
    "\n",
    "def get_accuracy_from_log_probs(log_probs, labels):\n",
    "    probs = torch.exp(log_probs)\n",
    "    predicted_label = torch.argmax(probs, dim=1)\n",
    "    acc = (predicted_label == labels).float().mean()\n",
    "    return acc\n",
    "\n",
    "# helper function to evaluate model on dev data\n",
    "def evaluate(model, criterion, dataloader, gpu):\n",
    "    model.eval()\n",
    "\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        dev_st = time.time()\n",
    "        for it, data_tensor in enumerate(dataloader):\n",
    "            context_tensor = data_tensor[:,0:CONTEXT_SIZE]\n",
    "            target_tensor = data_tensor[:,CONTEXT_SIZE]\n",
    "            context_tensor, target_tensor = context_tensor.cuda(gpu), target_tensor.cuda(gpu)\n",
    "            log_probs = model(context_tensor)\n",
    "            mean_loss += criterion(log_probs, target_tensor).item()\n",
    "            mean_acc += get_accuracy_from_log_probs(log_probs, target_tensor)\n",
    "            count += 1\n",
    "            if it % 500 == 0: \n",
    "                print(\"Dev Iteration {} complete. Mean Loss: {}; Mean Acc:{}; Time taken (s): {}\".format(it, mean_loss / count, mean_acc / count, (time.time()-dev_st)))\n",
    "                dev_st = time.time()\n",
    "\n",
    "    return mean_acc / count, mean_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1390a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# N-gram Neural Network Model\n",
    "class NgramNNmodel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, h):\n",
    "        super(NgramNNmodel, self).__init__()\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, h)\n",
    "        self.linear2 = nn.Linear(h, vocab_size, bias = False)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # compute x': concatenation of x1 and x2 embeddings\n",
    "        embeds = self.embeddings(inputs).view((-1,self.context_size * self.embedding_dim))\n",
    "        # compute h: tanh(W_1.x' + b)\n",
    "        out = torch.tanh(self.linear1(embeds))\n",
    "        # compute W_2.h\n",
    "        out = self.linear2(out)\n",
    "        # compute y: log_softmax(W_2.h)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        # return log probabilities\n",
    "        # BATCH_SIZE x len(vocab)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfe7fbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create parameters\n",
    "gpu = 0 \n",
    "# word vectors size\n",
    "EMBEDDING_DIM = 200\n",
    "CONTEXT_SIZE = 7\n",
    "BATCH_SIZE = 512\n",
    "# hidden units\n",
    "H = 100\n",
    "torch.manual_seed(42)\n",
    "learn_rate = 1e-3\n",
    "\n",
    "# check if gpu is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dbd6868",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = './data/wikitext-2'\n",
    "corpus = data.Corpus(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffda90f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = corpus.train\n",
    "val_set = corpus.valid\n",
    "test_set = corpus.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "212d4bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ngram = ngram_split(train_set, CONTEXT_SIZE)\n",
    "val_ngram = ngram_split(val_set, CONTEXT_SIZE)\n",
    "test_ngram = ngram_split(test_set, CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3a56d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ngram, batch_size = BATCH_SIZE)\n",
    "dev_loader = DataLoader(val_ngram, batch_size = BATCH_SIZE)\n",
    "test_loader = DataLoader(train_ngram, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "349a2db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using negative log-likelihood loss\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "vocab_len = len(corpus.dictionary)\n",
    "# vocab_len = len(vocab)\n",
    "\n",
    "# create model\n",
    "# Using negative log-likelihood loss\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "# create model\n",
    "model = NgramNNmodel(vocab_len, EMBEDDING_DIM, CONTEXT_SIZE, H)\n",
    "\n",
    "# load it to gpu\n",
    "model.cuda(gpu)\n",
    "\n",
    "# using ADAM optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr = learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a03d1cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training model Epoch: 1 ---\n",
      "Training Iteration 0 of epoch 0 complete. Loss: 10.441363334655762; Acc:0.0; Time taken (s): 0.6484103202819824\n",
      "Training Iteration 500 of epoch 0 complete. Loss: 6.789704322814941; Acc:0.115234375; Time taken (s): 14.784630060195923\n",
      "Training Iteration 1000 of epoch 0 complete. Loss: 6.175427436828613; Acc:0.158203125; Time taken (s): 14.68571662902832\n",
      "Training Iteration 1500 of epoch 0 complete. Loss: 6.106536865234375; Acc:0.18359375; Time taken (s): 14.696378707885742\n",
      "Training Iteration 2000 of epoch 0 complete. Loss: 5.956329822540283; Acc:0.15625; Time taken (s): 14.672945022583008\n",
      "Training Iteration 2500 of epoch 0 complete. Loss: 6.462583541870117; Acc:0.19140625; Time taken (s): 14.712609052658081\n",
      "Training Iteration 3000 of epoch 0 complete. Loss: 5.834971904754639; Acc:0.23046875; Time taken (s): 14.720713138580322\n",
      "Training Iteration 3500 of epoch 0 complete. Loss: 6.225164413452148; Acc:0.193359375; Time taken (s): 14.688605546951294\n",
      "Training Iteration 4000 of epoch 0 complete. Loss: 5.995873928070068; Acc:0.181640625; Time taken (s): 14.709143161773682\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 5.509657859802246; Mean Acc:0.197265625; Time taken (s): 0.015621423721313477\n",
      "Epoch 0 complete! Development Accuracy: 0.1832433044910431; Development Loss: 5.828506996374175; Perplexity: 339.8509014810279\n",
      "Best development accuracy improved from 999 to 339.8509014810279, saving model...\n",
      "\n",
      "--- Training model Epoch: 2 ---\n",
      "Training Iteration 0 of epoch 1 complete. Loss: 6.322117805480957; Acc:0.150390625; Time taken (s): 0.04668617248535156\n",
      "Training Iteration 500 of epoch 1 complete. Loss: 5.862503528594971; Acc:0.16015625; Time taken (s): 14.69665002822876\n",
      "Training Iteration 1000 of epoch 1 complete. Loss: 5.531418800354004; Acc:0.19921875; Time taken (s): 14.70386266708374\n",
      "Training Iteration 1500 of epoch 1 complete. Loss: 5.532200813293457; Acc:0.2109375; Time taken (s): 14.696882724761963\n",
      "Training Iteration 2000 of epoch 1 complete. Loss: 5.564194679260254; Acc:0.166015625; Time taken (s): 14.712815761566162\n",
      "Training Iteration 2500 of epoch 1 complete. Loss: 5.747562885284424; Acc:0.1875; Time taken (s): 14.848171949386597\n",
      "Training Iteration 3000 of epoch 1 complete. Loss: 5.388176918029785; Acc:0.240234375; Time taken (s): 14.711435794830322\n",
      "Training Iteration 3500 of epoch 1 complete. Loss: 5.8329854011535645; Acc:0.20703125; Time taken (s): 14.709360599517822\n",
      "Training Iteration 4000 of epoch 1 complete. Loss: 5.724244117736816; Acc:0.197265625; Time taken (s): 14.699704647064209\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 5.344829559326172; Mean Acc:0.20703125; Time taken (s): 0.015587329864501953\n",
      "Epoch 1 complete! Development Accuracy: 0.19374707341194153; Development Loss: 5.681313661902164; Perplexity: 293.33451931115036\n",
      "Best development accuracy improved from 339.8509014810279 to 293.33451931115036, saving model...\n",
      "\n",
      "--- Training model Epoch: 3 ---\n",
      "Training Iteration 0 of epoch 2 complete. Loss: 5.939806938171387; Acc:0.166015625; Time taken (s): 0.031218767166137695\n",
      "Training Iteration 500 of epoch 2 complete. Loss: 5.559309482574463; Acc:0.1640625; Time taken (s): 14.655114889144897\n",
      "Training Iteration 1000 of epoch 2 complete. Loss: 5.292018413543701; Acc:0.205078125; Time taken (s): 14.613969564437866\n",
      "Training Iteration 1500 of epoch 2 complete. Loss: 5.294645309448242; Acc:0.2265625; Time taken (s): 14.61924409866333\n",
      "Training Iteration 2000 of epoch 2 complete. Loss: 5.379818916320801; Acc:0.15625; Time taken (s): 14.608301877975464\n",
      "Training Iteration 2500 of epoch 2 complete. Loss: 5.470643997192383; Acc:0.203125; Time taken (s): 14.649831771850586\n",
      "Training Iteration 3000 of epoch 2 complete. Loss: 5.155824184417725; Acc:0.25; Time taken (s): 14.614418268203735\n",
      "Training Iteration 3500 of epoch 2 complete. Loss: 5.596273422241211; Acc:0.216796875; Time taken (s): 14.773908138275146\n",
      "Training Iteration 4000 of epoch 2 complete. Loss: 5.53259801864624; Acc:0.2109375; Time taken (s): 14.61150860786438\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 5.2782087326049805; Mean Acc:0.205078125; Time taken (s): 0.015621185302734375\n",
      "Epoch 2 complete! Development Accuracy: 0.1975662261247635; Development Loss: 5.628266814728858; Perplexity: 278.1795628543473\n",
      "Best development accuracy improved from 293.33451931115036 to 278.1795628543473, saving model...\n",
      "\n",
      "--- Training model Epoch: 4 ---\n",
      "Training Iteration 0 of epoch 3 complete. Loss: 5.701623916625977; Acc:0.1875; Time taken (s): 0.031222105026245117\n",
      "Training Iteration 500 of epoch 3 complete. Loss: 5.3709845542907715; Acc:0.181640625; Time taken (s): 14.723342895507812\n",
      "Training Iteration 1000 of epoch 3 complete. Loss: 5.134481430053711; Acc:0.220703125; Time taken (s): 15.80529499053955\n",
      "Training Iteration 1500 of epoch 3 complete. Loss: 5.148873329162598; Acc:0.2421875; Time taken (s): 17.876060009002686\n",
      "Training Iteration 2000 of epoch 3 complete. Loss: 5.249226093292236; Acc:0.1640625; Time taken (s): 15.13926649093628\n",
      "Training Iteration 2500 of epoch 3 complete. Loss: 5.274519920349121; Acc:0.21484375; Time taken (s): 16.212445974349976\n",
      "Training Iteration 3000 of epoch 3 complete. Loss: 4.974496841430664; Acc:0.259765625; Time taken (s): 15.39161992073059\n",
      "Training Iteration 3500 of epoch 3 complete. Loss: 5.4047532081604; Acc:0.240234375; Time taken (s): 15.019443035125732\n",
      "Training Iteration 4000 of epoch 3 complete. Loss: 5.3997015953063965; Acc:0.2109375; Time taken (s): 15.019730806350708\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 5.2409162521362305; Mean Acc:0.19921875; Time taken (s): 0.015620708465576172\n",
      "Epoch 3 complete! Development Accuracy: 0.20026208460330963; Development Loss: 5.608914331091402; Perplexity: 272.84785471416575\n",
      "Best development accuracy improved from 278.1795628543473 to 272.84785471416575, saving model...\n",
      "\n",
      "--- Training model Epoch: 5 ---\n",
      "Training Iteration 0 of epoch 4 complete. Loss: 5.522627830505371; Acc:0.1953125; Time taken (s): 0.04671931266784668\n",
      "Training Iteration 500 of epoch 4 complete. Loss: 5.242796897888184; Acc:0.193359375; Time taken (s): 15.043781518936157\n",
      "Training Iteration 1000 of epoch 4 complete. Loss: 5.004432678222656; Acc:0.23828125; Time taken (s): 15.003189325332642\n",
      "Training Iteration 1500 of epoch 4 complete. Loss: 5.026934623718262; Acc:0.251953125; Time taken (s): 15.038389205932617\n",
      "Training Iteration 2000 of epoch 4 complete. Loss: 5.1291823387146; Acc:0.1640625; Time taken (s): 15.074962377548218\n",
      "Training Iteration 2500 of epoch 4 complete. Loss: 5.116177082061768; Acc:0.216796875; Time taken (s): 15.014628648757935\n",
      "Training Iteration 3000 of epoch 4 complete. Loss: 4.805880546569824; Acc:0.267578125; Time taken (s): 15.035237312316895\n",
      "Training Iteration 3500 of epoch 4 complete. Loss: 5.255832195281982; Acc:0.2421875; Time taken (s): 15.023420810699463\n",
      "Training Iteration 4000 of epoch 4 complete. Loss: 5.275388717651367; Acc:0.21875; Time taken (s): 15.047231674194336\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 5.218466281890869; Mean Acc:0.205078125; Time taken (s): 0.015613079071044922\n",
      "Epoch 4 complete! Development Accuracy: 0.20157276093959808; Development Loss: 5.609074105119482; Perplexity: 272.89145219774645\n"
     ]
    }
   ],
   "source": [
    "# ------------------------- TRAIN & SAVE MODEL ------------------------\n",
    "best_acc = 0\n",
    "best_ppl = 999\n",
    "best_model_path = None\n",
    "for epoch in range(5):\n",
    "    st = time.time()\n",
    "    print(\"\\n--- Training model Epoch: {} ---\".format(epoch+1))\n",
    "    for it, data_tensor in enumerate(train_loader):\n",
    "        context_tensor = data_tensor[:,0:CONTEXT_SIZE]\n",
    "        target_tensor = data_tensor[:,CONTEXT_SIZE]\n",
    "#         print(context_tensor)\n",
    "#         print(target_tensor)\n",
    "        context_tensor, target_tensor = context_tensor.cuda(gpu), target_tensor.cuda(gpu)\n",
    "\n",
    "        # zero out the gradients from the old instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # get log probabilities over next words\n",
    "        log_probs = model(context_tensor)\n",
    "\n",
    "        # calculate current accuracy\n",
    "        acc = get_accuracy_from_log_probs(log_probs, target_tensor)\n",
    "\n",
    "        # compute loss function\n",
    "        loss = loss_function(log_probs, target_tensor)\n",
    "\n",
    "        # backward pass and update gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if it % 500 == 0: \n",
    "            print(\"Training Iteration {} of epoch {} complete. Loss: {}; Acc:{}; Time taken (s): {}\".format(it, epoch, loss.item(), acc, (time.time()-st)))\n",
    "            st = time.time()\n",
    "\n",
    "    print(\"\\n--- Evaluating model on dev data ---\")\n",
    "    dev_acc, dev_loss = evaluate(model, loss_function, dev_loader, gpu)\n",
    "    ppl = math.exp(dev_loss)\n",
    "    print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}; Perplexity: {}\".format(epoch, dev_acc, dev_loss, ppl))\n",
    "#    if dev_acc > best_acc:\n",
    "    if ppl < best_ppl:\n",
    "        print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_ppl, ppl))\n",
    "        best_ppl = ppl\n",
    "        # set best model path\n",
    "        best_model_path = 'best_model_{}_gram_{}.dat'.format(CONTEXT_SIZE+1, epoch)\n",
    "        # saving best model\n",
    "        torch.save(model.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b467ad01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5c0faa9",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2e3e086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(best_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3d41263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Iteration 0 complete. Mean Loss: 5.522627830505371; Mean Acc:0.1953125; Time taken (s): 0.01100301742553711\n",
      "Dev Iteration 500 complete. Mean Loss: 5.399713248787764; Mean Acc:0.2041035145521164; Time taken (s): 5.135958909988403\n",
      "Dev Iteration 1000 complete. Mean Loss: 5.437153098347423; Mean Acc:0.20221379399299622; Time taken (s): 4.940404176712036\n",
      "Dev Iteration 1500 complete. Mean Loss: 5.417639294280599; Mean Acc:0.20305734872817993; Time taken (s): 4.879214763641357\n",
      "Dev Iteration 2000 complete. Mean Loss: 5.396489654523858; Mean Acc:0.2040727734565735; Time taken (s): 4.904557466506958\n",
      "Dev Iteration 2500 complete. Mean Loss: 5.367625521927154; Mean Acc:0.20471186935901642; Time taken (s): 4.849256277084351\n",
      "Dev Iteration 3000 complete. Mean Loss: 5.342331157927115; Mean Acc:0.20545300841331482; Time taken (s): 4.96332311630249\n",
      "Dev Iteration 3500 complete. Mean Loss: 5.310218870418476; Mean Acc:0.20649290084838867; Time taken (s): 5.685816526412964\n",
      "Dev Iteration 4000 complete. Mean Loss: 5.259702313515402; Mean Acc:0.20883256196975708; Time taken (s): 4.8553595542907715\n"
     ]
    }
   ],
   "source": [
    "test_acc, test_loss = evaluate(model, loss_function, test_loader, gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e845c356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 190.15509964467603\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Perplexity:\", math.exp(test_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
