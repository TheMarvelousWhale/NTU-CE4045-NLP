{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16761e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "import data_fnn as data\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "574feee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_split(dataset, n):\n",
    "    # This function breaks corpus into [context, target]\n",
    "    # For e.g., in trigram, the tensor returned would be [C(n-2), C(n-1), T]\n",
    "    ngram = []\n",
    "    data_len = len(dataset)\n",
    "    for i, tokenid in enumerate(dataset):\n",
    "        if i+n<data_len:\n",
    "            ngram.append(dataset[i:i+n+1].view(-1))\n",
    "    fin_ngram=torch.stack(ngram)\n",
    "    return fin_ngram\n",
    "\n",
    "def get_accuracy_from_log_probs(log_probs, labels):\n",
    "    probs = torch.exp(log_probs)\n",
    "    predicted_label = torch.argmax(probs, dim=1)\n",
    "    acc = (predicted_label == labels).float().mean()\n",
    "    return acc\n",
    "\n",
    "# helper function to evaluate model on dev data\n",
    "def evaluate(model, criterion, dataloader, gpu):\n",
    "    model.eval()\n",
    "\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        dev_st = time.time()\n",
    "        for it, data_tensor in enumerate(dataloader):\n",
    "            context_tensor = data_tensor[:,0:CONTEXT_SIZE]\n",
    "            target_tensor = data_tensor[:,CONTEXT_SIZE]\n",
    "            context_tensor, target_tensor = context_tensor.cuda(gpu), target_tensor.cuda(gpu)\n",
    "            log_probs = model(context_tensor)\n",
    "            mean_loss += criterion(log_probs, target_tensor).item()\n",
    "            mean_acc += get_accuracy_from_log_probs(log_probs, target_tensor)\n",
    "            count += 1\n",
    "            if it % 500 == 0: \n",
    "                print(\"Dev Iteration {} complete. Mean Loss: {}; Mean Acc:{}; Time taken (s): {}\".format(it, mean_loss / count, mean_acc / count, (time.time()-dev_st)))\n",
    "                dev_st = time.time()\n",
    "\n",
    "    return mean_acc / count, mean_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfe7fbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create parameters\n",
    "gpu = 0 \n",
    "# word vectors size\n",
    "EMBEDDING_DIM = 200\n",
    "CONTEXT_SIZE = 7\n",
    "BATCH_SIZE = 512\n",
    "shared = True\n",
    "# hidden units\n",
    "if shared:\n",
    "    H = 200\n",
    "else:\n",
    "    H = 100\n",
    "torch.manual_seed(42)\n",
    "learn_rate = 1e-3\n",
    "\n",
    "# check if gpu is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dbd6868",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = './data/wikitext-2'\n",
    "corpus = data.Corpus(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffda90f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = corpus.train\n",
    "val_set = corpus.valid\n",
    "test_set = corpus.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdcd2217-adaf-4a21-91fb-d546d6f0d655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robert robert is an english film , television and theatre actor . he had a guest - starring role on the television series the bill in 2000 . this was followed by a starring role in the play herons written by simon stephens , which was performed in 2001 at the royal court theatre . he had a guest role in the television series judge john in 2002 . in 2004 landed a role as craig in the episode teddy s story of the television series the long firm he starred alongside actors mark strong and derek jacobi . he\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([corpus.dictionary.idx2word[i] for i in test_set[:100]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "212d4bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ngram = ngram_split(train_set, CONTEXT_SIZE)\n",
    "val_ngram = ngram_split(val_set, CONTEXT_SIZE)\n",
    "test_ngram = ngram_split(test_set, CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3a56d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ngram, batch_size = BATCH_SIZE)\n",
    "dev_loader = DataLoader(val_ngram, batch_size = BATCH_SIZE)\n",
    "test_loader = DataLoader(train_ngram, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "349a2db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using negative log-likelihood loss\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "vocab_len = len(corpus.dictionary)\n",
    "# vocab_len = len(vocab)\n",
    "\n",
    "# create model\n",
    "# Using negative log-likelihood loss\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "# create model\n",
    "model = model.FNNModel(vocab_len, EMBEDDING_DIM, CONTEXT_SIZE, H)\n",
    "\n",
    "# load it to gpu\n",
    "model.cuda(gpu)\n",
    "\n",
    "# using ADAM optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr = learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03d1cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training model Epoch: 1 ---\n",
      "Training Iteration 0 of epoch 0 complete. Loss: 4.931563854217529; Acc:0.19921875; Time taken (s): 0.050102949142456055\n",
      "Training Iteration 500 of epoch 0 complete. Loss: 4.8811774253845215; Acc:0.216796875; Time taken (s): 16.554916381835938\n",
      "Training Iteration 1000 of epoch 0 complete. Loss: 5.077020645141602; Acc:0.205078125; Time taken (s): 16.584932565689087\n",
      "Training Iteration 1500 of epoch 0 complete. Loss: 4.665365219116211; Acc:0.248046875; Time taken (s): 16.615304231643677\n",
      "Training Iteration 2000 of epoch 0 complete. Loss: 4.7349677085876465; Acc:0.216796875; Time taken (s): 16.624900579452515\n",
      "Training Iteration 2500 of epoch 0 complete. Loss: 5.052066802978516; Acc:0.15625; Time taken (s): 16.57492423057556\n",
      "Training Iteration 3000 of epoch 0 complete. Loss: 4.780981540679932; Acc:0.234375; Time taken (s): 16.654953718185425\n",
      "Training Iteration 3500 of epoch 0 complete. Loss: 4.909718036651611; Acc:0.19140625; Time taken (s): 16.612359285354614\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 5.921858787536621; Mean Acc:0.146484375; Time taken (s): 0.020282983779907227\n",
      "Epoch 0 complete! Development Accuracy: 0.17103584110736847; Development Loss: 6.155263662338257; Perplexity: 471.1910614211973\n",
      "Just save it\n",
      "Best development accuracy improved from 999 to 471.1910614211973, saving model...\n",
      "\n",
      "--- Training model Epoch: 2 ---\n",
      "Training Iteration 0 of epoch 1 complete. Loss: 4.784249782562256; Acc:0.212890625; Time taken (s): 0.030098438262939453\n",
      "Training Iteration 500 of epoch 1 complete. Loss: 4.683418273925781; Acc:0.212890625; Time taken (s): 16.664541721343994\n",
      "Training Iteration 1000 of epoch 1 complete. Loss: 4.911529064178467; Acc:0.216796875; Time taken (s): 16.739981174468994\n",
      "Training Iteration 1500 of epoch 1 complete. Loss: 4.511430263519287; Acc:0.251953125; Time taken (s): 16.725083351135254\n",
      "Training Iteration 2000 of epoch 1 complete. Loss: 4.58560848236084; Acc:0.220703125; Time taken (s): 16.740221977233887\n",
      "Training Iteration 2500 of epoch 1 complete. Loss: 4.8197126388549805; Acc:0.17578125; Time taken (s): 16.71465826034546\n",
      "Training Iteration 3000 of epoch 1 complete. Loss: 4.586241722106934; Acc:0.24609375; Time taken (s): 16.684772968292236\n",
      "Training Iteration 3500 of epoch 1 complete. Loss: 4.767341136932373; Acc:0.208984375; Time taken (s): 16.670069217681885\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 6.000168800354004; Mean Acc:0.14453125; Time taken (s): 0.02205371856689453\n",
      "Epoch 1 complete! Development Accuracy: 0.17014095187187195; Development Loss: 6.217348453846384; Perplexity: 501.37205681513683\n",
      "Just save it\n",
      "Best development accuracy improved from 471.1910614211973 to 501.37205681513683, saving model...\n",
      "\n",
      "--- Training model Epoch: 3 ---\n",
      "Training Iteration 0 of epoch 2 complete. Loss: 4.657010078430176; Acc:0.23046875; Time taken (s): 0.03498053550720215\n",
      "Training Iteration 500 of epoch 2 complete. Loss: 4.502945899963379; Acc:0.212890625; Time taken (s): 16.715025424957275\n",
      "Training Iteration 1000 of epoch 2 complete. Loss: 4.757924556732178; Acc:0.224609375; Time taken (s): 16.66483163833618\n",
      "Training Iteration 1500 of epoch 2 complete. Loss: 4.36836051940918; Acc:0.263671875; Time taken (s): 16.654736757278442\n",
      "Training Iteration 2000 of epoch 2 complete. Loss: 4.454903602600098; Acc:0.228515625; Time taken (s): 16.94197654724121\n",
      "Training Iteration 2500 of epoch 2 complete. Loss: 4.615981578826904; Acc:0.193359375; Time taken (s): 16.756836891174316\n",
      "Training Iteration 3000 of epoch 2 complete. Loss: 4.415806770324707; Acc:0.26171875; Time taken (s): 16.73208451271057\n",
      "Training Iteration 3500 of epoch 2 complete. Loss: 4.644222736358643; Acc:0.22265625; Time taken (s): 16.768974542617798\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 6.095471382141113; Mean Acc:0.150390625; Time taken (s): 0.019998550415039062\n",
      "Epoch 2 complete! Development Accuracy: 0.1692163348197937; Development Loss: 6.283252131431661; Perplexity: 535.5274405497779\n",
      "Just save it\n",
      "Best development accuracy improved from 501.37205681513683 to 535.5274405497779, saving model...\n",
      "\n",
      "--- Training model Epoch: 4 ---\n",
      "Training Iteration 0 of epoch 3 complete. Loss: 4.549615859985352; Acc:0.240234375; Time taken (s): 0.03507685661315918\n",
      "Training Iteration 500 of epoch 3 complete. Loss: 4.331848621368408; Acc:0.2265625; Time taken (s): 16.949647665023804\n",
      "Training Iteration 1000 of epoch 3 complete. Loss: 4.621453285217285; Acc:0.2421875; Time taken (s): 16.77198362350464\n",
      "Training Iteration 1500 of epoch 3 complete. Loss: 4.227752208709717; Acc:0.271484375; Time taken (s): 16.75773549079895\n",
      "Training Iteration 2000 of epoch 3 complete. Loss: 4.334024906158447; Acc:0.244140625; Time taken (s): 16.7883358001709\n"
     ]
    }
   ],
   "source": [
    "# ------------------------- TRAIN & SAVE MODEL ------------------------\n",
    "best_acc = 0\n",
    "best_ppl = 999\n",
    "best_model_path = None\n",
    "for epoch in range(10):\n",
    "    st = time.time()\n",
    "    print(\"\\n--- Training model Epoch: {} ---\".format(epoch+1))\n",
    "    for it, data_tensor in enumerate(train_loader):\n",
    "        context_tensor = data_tensor[:,0:CONTEXT_SIZE]\n",
    "        target_tensor = data_tensor[:,CONTEXT_SIZE]\n",
    "#         print(context_tensor)\n",
    "#         print(target_tensor)\n",
    "        context_tensor, target_tensor = context_tensor.cuda(gpu), target_tensor.cuda(gpu)\n",
    "\n",
    "        # zero out the gradients from the old instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # get log probabilities over next words\n",
    "        log_probs = model(context_tensor)\n",
    "        # calculate current accuracy\n",
    "        acc = get_accuracy_from_log_probs(log_probs, target_tensor)\n",
    "\n",
    "        # compute loss function\n",
    "        loss = loss_function(log_probs, target_tensor)\n",
    "\n",
    "        # backward pass and update gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if it % 500 == 0: \n",
    "            print(\"Training Iteration {} of epoch {} complete. Loss: {}; Acc:{}; Time taken (s): {}\".format(it, epoch, loss.item(), acc, (time.time()-st)))\n",
    "            st = time.time()\n",
    "\n",
    "    print(\"\\n--- Evaluating model on dev data ---\")\n",
    "    dev_acc, dev_loss = evaluate(model, loss_function, dev_loader, gpu)\n",
    "    ppl = math.exp(dev_loss)\n",
    "    print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}; Perplexity: {}\".format(epoch, dev_acc, dev_loss, ppl))\n",
    "#    if dev_acc > best_acc:\n",
    "    print(\"Just save it\")\n",
    "    print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_ppl, ppl))\n",
    "    best_ppl = ppl\n",
    "    # set best model path\n",
    "    best_model_path = 'debug_best_model_{}_gram_{}_{}H.dat'.format(CONTEXT_SIZE+1, epoch, H)\n",
    "    # saving best model\n",
    "    torch.save(model.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b467ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'debug_'+best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c0faa9",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e3e086",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(best_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d41263",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, test_loss = evaluate(model, loss_function, test_loader, gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e845c356",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Perplexity:\", math.exp(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca5ce19",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model_shared.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc5f032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b370d5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c23ce628",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de12304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import model\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dda230",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data.Corpus('./data/wikitext-2')\n",
    "ntokens = len(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c953d95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec73da18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'model_shared.pt'\n",
    "device = torch.device(\"cuda\")\n",
    "model = torch.load(model_path)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc47a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = dict(model.named_children())\n",
    "embedding_size = l['embeddings'].embedding_dim # 200\n",
    "input_layer_dim = l['linear1'].in_features # 1400\n",
    "context_size = int(input_layer_dim/embedding_size) # 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc48dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_corpus = torch.cat((corpus.train, corpus.valid, corpus.test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d10a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_pos = random.randint(0, len(full_corpus)-context_size)\n",
    "seed_span = full_corpus[seed_pos:seed_pos+context_size] # Pick random span from corpus\n",
    "generated_text=seed_span.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39313ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for i in range(15):\n",
    "    with torch.no_grad():\n",
    "        output = model(generated_text[-7:])\n",
    "        word_id = torch.argmax(output, dim=1)\n",
    "        generated_text = torch.cat((generated_text,word_id))\n",
    "        print(generated_text[-8:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4926234",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = generated_text.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e95e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, i in enumerate(generated_text):\n",
    "    if index == 7: \n",
    "        print(\" | \", end = \"\")\n",
    "    print(corpus.dictionary.idx2word[i], end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20101a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5389cb14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f5ee2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WhyAmIStillAlive",
   "language": "python",
   "name": "tor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
