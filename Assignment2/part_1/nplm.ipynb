{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5afa6c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "# from nltk.corpus import brown\n",
    "# from nltk.corpus import wordnet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16761e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download(\"brown\")\n",
    "# nltk.download(\"wordnet\")\n",
    "\n",
    "# len(brown.paras())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1390a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import multiprocessing\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "# Trigram Neural Network Model\n",
    "class TrigramNNmodel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, h):\n",
    "        super(TrigramNNmodel, self).__init__()\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, h)\n",
    "        self.linear2 = nn.Linear(h, vocab_size, bias = False)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # compute x': concatenation of x1 and x2 embeddings\n",
    "        embeds = self.embeddings(inputs).view((-1,self.context_size * self.embedding_dim))\n",
    "        # compute h: tanh(W_1.x' + b)\n",
    "        out = torch.tanh(self.linear1(embeds))\n",
    "        # compute W_2.h\n",
    "        out = self.linear2(out)\n",
    "        # compute y: log_softmax(W_2.h)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        # return log probabilities\n",
    "        # BATCH_SIZE x len(vocab)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfe7fbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create parameters\n",
    "gpu = 0 \n",
    "# word vectors size\n",
    "EMBEDDING_DIM = 200\n",
    "CONTEXT_SIZE = 8\n",
    "BATCH_SIZE = 256\n",
    "# hidden units\n",
    "H = 100\n",
    "torch.manual_seed(13013)\n",
    "\n",
    "# check if gpu is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "available_workers = multiprocessing.cpu_count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d30217d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# num_train = 12000\n",
    "# UNK_symbol = \"<UNK>\"\n",
    "# vocab = set([UNK_symbol])\n",
    "\n",
    "# # create brown corpus again with all words\n",
    "# # no preprocessing, only lowercase\n",
    "# brown_corpus_train = []\n",
    "# for idx,paragraph in enumerate(brown.paras()):\n",
    "#     if idx == num_train:\n",
    "#         break\n",
    "#     words = []\n",
    "#     for sentence in paragraph:\n",
    "#         for word in sentence:\n",
    "#             words.append(word.lower())\n",
    "#     brown_corpus_train.append(words)\n",
    "\n",
    "# # create term frequency of the words\n",
    "# words_term_frequency_train = {}\n",
    "# for doc in brown_corpus_train:\n",
    "#     for word in doc:\n",
    "#         # this will calculate term frequency\n",
    "#         # since we are taking all words now\n",
    "#         words_term_frequency_train[word] = words_term_frequency_train.get(word,0) + 1\n",
    "\n",
    "# # create vocabulary\n",
    "# for doc in brown_corpus_train:\n",
    "#     for word in doc:\n",
    "#         if words_term_frequency_train.get(word,0) >= 5:\n",
    "#             vocab.add(word)\n",
    "\n",
    "# print(len(vocab))\n",
    "\n",
    "# # create required lists\n",
    "# x_train = []\n",
    "# y_train = []\n",
    "# x_dev = []\n",
    "# y_dev = []\n",
    "\n",
    "# # create word to id mappings\n",
    "# word_to_id_mappings = {}\n",
    "# for idx,word in enumerate(vocab):\n",
    "#     word_to_id_mappings[word] = idx\n",
    "\n",
    "# # function to get id for a given word\n",
    "# # return <UNK> id if not found\n",
    "# def get_id_of_word(word):\n",
    "#     unknown_word_id = word_to_id_mappings['<UNK>']\n",
    "#     return word_to_id_mappings.get(word,unknown_word_id)\n",
    "\n",
    "# # creating training and dev set\n",
    "# for idx,paragraph in enumerate(brown.paras()):\n",
    "#     for sentence in paragraph:\n",
    "#         for i,word in enumerate(sentence):\n",
    "#             if i+CONTEXT_SIZE >= len(sentence):\n",
    "#                 # sentence boundary reached\n",
    "#                 # ignoring sentence less than 3 words\n",
    "#                 break\n",
    "#             # convert word to id\n",
    "#             x_extract = [get_id_of_word(word.lower()),get_id_of_word(sentence[i+1].lower())]\n",
    "#             y_extract = [get_id_of_word(sentence[i+2].lower())]\n",
    "#             if idx < num_train:\n",
    "#                 x_train.append(x_extract)\n",
    "#                 y_train.append(y_extract)\n",
    "#             else:\n",
    "#                 x_dev.append(x_extract)\n",
    "#                 y_dev.append(y_extract)\n",
    "\n",
    "# # making numpy arrays\n",
    "# x_train = np.array(x_train)\n",
    "# y_train = np.array(y_train)\n",
    "# x_dev = np.array(x_dev)\n",
    "# y_dev = np.array(y_dev)  \n",
    "  \n",
    "# print(x_train.shape)\n",
    "# print(y_train.shape)\n",
    "# print(x_dev.shape)\n",
    "# print(y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "668994fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(\"--- Creating training and dev dataloaders with {} batch size ---\".format(BATCH_SIZE))\n",
    "# train_set = np.concatenate((x_train, y_train), axis=1)\n",
    "# dev_set = np.concatenate((x_dev, y_dev), axis=1)\n",
    "# train_loader = DataLoader(train_set, batch_size = BATCH_SIZE, num_workers = 4)\n",
    "# dev_loader = DataLoader(dev_set, batch_size = BATCH_SIZE, num_workers = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a3a94a",
   "metadata": {},
   "source": [
    "### Start of wikitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4000bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dbd6868",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = './data/wikitext-2'\n",
    "corpus = data.Corpus(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffda90f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = corpus.train\n",
    "val_set = corpus.valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3522e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_split(dataset, n):\n",
    "    # This function breaks corpus into [context, target]\n",
    "    # For e.g., in trigram, the tensor returned would be [C(n-2), C(n-1), T]\n",
    "    ngram = []\n",
    "    data_len = len(dataset)\n",
    "    for i, tokenid in enumerate(dataset):\n",
    "        if i+n<data_len:\n",
    "            ngram.append(dataset[i:i+n+1].view(-1))\n",
    "    fin_ngram=torch.stack(ngram)\n",
    "    return fin_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "212d4bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ngram = ngram_split(train_set, CONTEXT_SIZE)\n",
    "val_ngram = ngram_split(val_set, CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3a56d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ngram, batch_size = BATCH_SIZE)\n",
    "dev_loader = DataLoader(val_ngram, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce665703",
   "metadata": {},
   "source": [
    "### End of wikitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe87c23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_from_log_probs(log_probs, labels):\n",
    "    probs = torch.exp(log_probs)\n",
    "    predicted_label = torch.argmax(probs, dim=1)\n",
    "    acc = (predicted_label == labels).float().mean()\n",
    "    return acc\n",
    "\n",
    "# helper function to evaluate model on dev data\n",
    "def evaluate(model, criterion, dataloader, gpu):\n",
    "    model.eval()\n",
    "\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        dev_st = time.time()\n",
    "        for it, data_tensor in enumerate(dataloader):\n",
    "            context_tensor = data_tensor[:,0:CONTEXT_SIZE]\n",
    "            target_tensor = data_tensor[:,CONTEXT_SIZE]\n",
    "            context_tensor, target_tensor = context_tensor.cuda(gpu), target_tensor.cuda(gpu)\n",
    "            log_probs = model(context_tensor)\n",
    "            mean_loss += criterion(log_probs, target_tensor).item()\n",
    "            mean_acc += get_accuracy_from_log_probs(log_probs, target_tensor)\n",
    "            count += 1\n",
    "            if it % 500 == 0: \n",
    "                print(\"Dev Iteration {} complete. Mean Loss: {}; Mean Acc:{}; Time taken (s): {}\".format(it, mean_loss / count, mean_acc / count, (time.time()-dev_st)))\n",
    "                dev_st = time.time()\n",
    "\n",
    "    return mean_acc / count, mean_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a03d1cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training model Epoch: 1 ---\n",
      "Training Iteration 0 of epoch 0 complete. Loss: 10.419032096862793; Acc:0.0; Time taken (s): 0.36246657371520996\n",
      "Training Iteration 500 of epoch 0 complete. Loss: 6.278597354888916; Acc:0.15625; Time taken (s): 10.70093822479248\n",
      "Training Iteration 1000 of epoch 0 complete. Loss: 6.794188022613525; Acc:0.09765625; Time taken (s): 10.582939386367798\n",
      "Training Iteration 1500 of epoch 0 complete. Loss: 6.001385688781738; Acc:0.1640625; Time taken (s): 11.136458396911621\n",
      "Training Iteration 2000 of epoch 0 complete. Loss: 5.9846954345703125; Acc:0.15234375; Time taken (s): 10.64143180847168\n",
      "Training Iteration 2500 of epoch 0 complete. Loss: 6.174255847930908; Acc:0.1640625; Time taken (s): 10.979711294174194\n",
      "Training Iteration 3000 of epoch 0 complete. Loss: 5.798740386962891; Acc:0.1796875; Time taken (s): 10.783837080001831\n",
      "Training Iteration 3500 of epoch 0 complete. Loss: 6.755301475524902; Acc:0.09765625; Time taken (s): 10.784493684768677\n",
      "Training Iteration 4000 of epoch 0 complete. Loss: 5.917647838592529; Acc:0.1640625; Time taken (s): 10.967804908752441\n",
      "Training Iteration 4500 of epoch 0 complete. Loss: 6.009994983673096; Acc:0.171875; Time taken (s): 10.923116207122803\n",
      "Training Iteration 5000 of epoch 0 complete. Loss: 6.849010467529297; Acc:0.14453125; Time taken (s): 10.650735855102539\n",
      "Training Iteration 5500 of epoch 0 complete. Loss: 6.708720684051514; Acc:0.19921875; Time taken (s): 10.629425525665283\n",
      "Training Iteration 6000 of epoch 0 complete. Loss: 5.663235187530518; Acc:0.23046875; Time taken (s): 10.845771312713623\n",
      "Training Iteration 6500 of epoch 0 complete. Loss: 5.889065742492676; Acc:0.19921875; Time taken (s): 11.159290313720703\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15164/209497230.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n--- Training model Epoch: {} ---\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_tensor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mcontext_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_tensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mCONTEXT_SIZE\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mtarget_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_tensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mCONTEXT_SIZE\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\4045a2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\4045a2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\4045a2\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\4045a2\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'numpy'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'string_'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Using negative log-likelihood loss\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "vocab_len = len(corpus.dictionary)\n",
    "# vocab_len = len(vocab)\n",
    "\n",
    "# create model\n",
    "# Using negative log-likelihood loss\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "# create model\n",
    "model = TrigramNNmodel(vocab_len, EMBEDDING_DIM, CONTEXT_SIZE, H)\n",
    "\n",
    "# load it to gpu\n",
    "model.cuda(gpu)\n",
    "\n",
    "# using ADAM optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr = 2e-3)\n",
    "\n",
    "\n",
    "# ------------------------- TRAIN & SAVE MODEL ------------------------\n",
    "best_acc = 0\n",
    "best_model_path = None\n",
    "for epoch in range(5):\n",
    "    st = time.time()\n",
    "    print(\"\\n--- Training model Epoch: {} ---\".format(epoch+1))\n",
    "    for it, data_tensor in enumerate(train_loader):\n",
    "        context_tensor = data_tensor[:,0:CONTEXT_SIZE]\n",
    "        target_tensor = data_tensor[:,CONTEXT_SIZE]\n",
    "#         print(context_tensor)\n",
    "#         print(target_tensor)\n",
    "        context_tensor, target_tensor = context_tensor.cuda(gpu), target_tensor.cuda(gpu)\n",
    "\n",
    "        # zero out the gradients from the old instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # get log probabilities over next words\n",
    "        log_probs = model(context_tensor)\n",
    "\n",
    "        # calculate current accuracy\n",
    "        acc = get_accuracy_from_log_probs(log_probs, target_tensor)\n",
    "\n",
    "        # compute loss function\n",
    "        loss = loss_function(log_probs, target_tensor)\n",
    "\n",
    "        # backward pass and update gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if it % 500 == 0: \n",
    "            print(\"Training Iteration {} of epoch {} complete. Loss: {}; Acc:{}; Time taken (s): {}\".format(it, epoch, loss.item(), acc, (time.time()-st)))\n",
    "            st = time.time()\n",
    "\n",
    "    print(\"\\n--- Evaluating model on dev data ---\")\n",
    "    dev_acc, dev_loss = evaluate(model, loss_function, dev_loader, gpu)\n",
    "    print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}\".format(epoch, dev_acc, dev_loss))\n",
    "    if dev_acc > best_acc:\n",
    "        print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_acc, dev_acc))\n",
    "        best_acc = dev_acc\n",
    "        # set best model path\n",
    "        best_model_path = 'best_model_{}.dat'.format(epoch)\n",
    "        # saving best model\n",
    "        torch.save(model.state_dict(), best_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
